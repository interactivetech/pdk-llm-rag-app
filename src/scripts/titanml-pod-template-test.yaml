apiVersion: v1
kind: Pod
metadata:
  name: titanml-pod
  labels:
    name: titanml-pod
spec:
  containers:
  - name: ubuntu
    image: mendeza/takeoff-mistral
    command: ["/bin/sh", "-c"]
    args:
      - echo starting;
        export TAKEOFF_MODEL_NAME=/code/models/mistralai/Mistral-7B-Instruct-v0.1;
        export TAKEOFF_DEVICE=cuda;
        sh run.sh
    ports:
    - containerPort: 80
      hostPort: 80
    volumeMounts:
    - name: host-volume
      mountPath: /code/models/
    - name: host-volume2
      mountPath: /nvmefs1/
    resources:
      limits:
        nvidia.com/gpu: 1 #specify number of gpus required
  tolerations:
    - effect: NoSchedule
      key: accelerator
      operator: Equal
      value: Tesla-T4
  volumes:
  - name: host-volume
    hostPath:
      path: /nvmefs1/andrew.mendez/titanml_cache
  - name: host-volume2
    hostPath:
      path: /nvmefs1/
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: titanml-pod-svc
  name: titanml-pod-svc
spec:
  type: LoadBalancer
  ports:
  - nodePort: 31174
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    name: titanml-pod
  sessionAffinity: None
  type: LoadBalancer
  loadBalancerIP: 10.182.1.48
