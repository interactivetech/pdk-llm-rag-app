name: mistral_finetune_hpe
workspace: Andrew
project: Mixtral
description: "Mixtral"

hyperparameters:
    global_batch_size: 1
    weight_decay: 0.001
    learning_rate: 0.00005
    adam_epsilon: 1e-8
    warmup_steps: 3
    gradient_accumulation_steps: 1
    # dataset_name: 'PDS2'
    model_path: '/nvmefs1/andrew.mendez/mixtral/model_and_tokenizer'
    tokenizer_path: '/nvmefs1/andrew.mendez/mixtral/model_and_tokenizer'
    finetune_results_dir: '/nvmefs1/andrew.mendez/mixtral_ckpt/'
    dataset_name: 'hpe_press_release'
bind_mounts:
  - container_path: /nvmefs1/
    host_path: /nvmefs1/
  - container_path: /pfs/
    host_path: /pfs/
environment:
    image: "mendeza/mixtral-rag-env-train:0.0.1"
records_per_epoch: 64 
resources:
    slots_per_trial: 1
    resource_pool: A100
min_validation_period:
  batches: 2
checkpoint_policy: none
checkpoint_storage:
  host_path: /nvmefs1/determined/checkpoints
  propagation: rprivate
  save_experiment_best: 0
  save_trial_best: 0
  save_trial_latest: 0
  storage_path: null
  type: shared_fs
profiling:
  begin_on_batch: 0
  enabled: true
min_checkpoint_period:
  batches: 2
searcher:
    name: single
    metric: eval_loss
    max_length:
        batches: 2
    smaller_is_better: true
max_restarts: 0
entrypoint: python3 -m determined.launch.torch_distributed --trial model_def_mixtral:MixtralFinetuneTrial