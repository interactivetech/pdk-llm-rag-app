{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04f8c659-8f96-4d64-9e08-64b4cb849647",
   "metadata": {},
   "source": [
    "# Chunking to support multi-turn chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78fcdb-313a-44a8-abf6-4380cc048992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import argparse\n",
    "\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "from chromadb.api.types import (\n",
    "    Document,\n",
    "    Documents,\n",
    "    Embedding,\n",
    "    Image,\n",
    "    Images,\n",
    "    EmbeddingFunction,\n",
    "    Embeddings,\n",
    "    is_image,\n",
    "    is_document,\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd95235a-1baf-4059-876c-8436ff307f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff902e57-3be4-40d8-bba4-2cd5e62e7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'hpe_press_releases.csv'\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b5a7c-14aa-48d8-9f7c-8232e1c9bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Content'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d17cb4-c484-4190-a636-bf70c1779245",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5208243-838b-4b97-b520-12dd38b922b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = text_splitter.split_text(df['Content'].iloc[0])\n",
    "# [(i,o) for i,o in zip(l,[df['Content'].iloc[0]])][0]\n",
    "def chunk_text(text_splitter,text):\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# chunk_text(text_splitter,df['Content'].iloc[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c84130-92db-458a-87f9-26d5d406dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = data_path\n",
    "df = pd.read_csv(data_path)\n",
    "LEN=df.shape[0]\n",
    "chunks = []\n",
    "inds = []\n",
    "for i in range(LEN):\n",
    "    ch = chunk_text(text_splitter,df.iloc[i]['Content'])\n",
    "    chunks+=ch\n",
    "    inds+=[i]*len(ch)\n",
    "\n",
    "print(len(chunks),len(inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ead98c-eb9e-4e49-86c5-caa3ae75d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = chromadb.get_settings()\n",
    "settings.allow_reset = True\n",
    "path_to_db='/nvmefs1/test_user/cache/rag_db2/'\n",
    "# model_path='/nvmefs1/andrew.mendez/chromadb_cache/all-MiniLM-L6-v2'\n",
    "model_path='/nvmefs1/test_user/cache/vector_model/e5-base-v2'\n",
    "print(f\"creating/resetting db at {path_to_db}...\")\n",
    "db = chromadb.PersistentClient(path=path_to_db, settings=settings)\n",
    "print(\"Done!\")\n",
    "db.reset()\n",
    "# model_path=args.emb_model_path\n",
    "print(\"Loading {}...\".format(model_path))\n",
    "emb_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=model_path, device=\"cuda\"\n",
    ")\n",
    "print(\"Done!\")\n",
    "#model_path='/mnt/efs/shared_fs/determined/all-MiniLM-L6-v2/'\n",
    "#emb_fn.models['all-MiniLM-L6-v2'].save(model_path)\n",
    "#print(\"Model saved at:{} \".format(model_path))\n",
    "collection = db.create_collection(name=\"HPE_press_releases\", embedding_function=emb_fn)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=8000, chunk_overlap=100)\n",
    "def chunk_text(text_splitter,text):\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# data_path = csv_path\n",
    "df = pd.read_csv(data_path)\n",
    "LEN=df.shape[0]\n",
    "chunks = []\n",
    "inds = []\n",
    "print(\"Number of docs: \",LEN)\n",
    "for i in tqdm(range(LEN)):\n",
    "    ch = chunk_text(text_splitter,df.iloc[i]['Content'])\n",
    "    chunks+=ch\n",
    "    inds+=[i]*len(ch)\n",
    "print(\"Number of chunks: \",len(chunks))\n",
    "\n",
    "for i in tqdm(range(len(chunks))):\n",
    "    collection.add(\n",
    "        documents=[chunks[i]],\n",
    "        metadatas=[{'Title':df.iloc[inds[i]]['Title'],'Content':df.iloc[inds[i]]['Content'],'Date':df.iloc[inds[i]]['Date']}],\n",
    "        ids=[f'id{str(i)}']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4306cd7-7a94-426a-a7f0-fcfa4e3a9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172843b5-9fa0-4583-bede-bd666d8a1a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How long has Antonio Neri been at HPE?\"\n",
    "# query = \"Who is Antonio Neri?\"\n",
    "# query = \"What is HPE Greenlake for Large Language Models?\"\n",
    "\n",
    "results = collection.query(query_texts=[query], n_results=2)\n",
    "print(\"query: \",query, \"results: \",results['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605dceaa-36b3-4298-8e58-1eaa1370d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68253272-169f-463d-8a74-0a959d41d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = \"\\n\".join([results['documents'][0][0]])\n",
    "results2 = results2[:4500]\n",
    "print(\"len(results2): \",len(results2))\n",
    "print(\"results2: \",results2)\n",
    "prompt = f\"[INST]`{results2}`. Using the above information, answer the following question factually: {query}. Answer concisely at most in three sentences. Respond in a natural way, like you are having a conversation with a friend.[/INST]\"\n",
    "print(\"=========prompt=============: \")\n",
    "print(prompt)\n",
    "print(\"=========end_of_prompt=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d07dbd-0230-4e5b-b538-38341fbb1e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f1e0f-36d1-45ae-ac11-0039e7a570f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa59b1d-7a89-4da7-935e-35ad85dbf348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [results['metadatas'][0][i]['Content'] for i in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba60cbc-0227-4ee9-93e3-dd20ccd39bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2 = \"\\n\\n\".join([results['metadatas'][0][0]['Content']])\n",
    "# results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222632b-90f0-4d33-9fd2-1d420ea8ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['documents'][0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e416e-8d0f-4460-aa95-d604867bf202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2 = \"\\n\\n\".join([results['metadatas'][0][0]['Content']])\n",
    "# print(\"8500//3: \",8500//3)\n",
    "results2 = \"\\n\".join([i for i in results['documents'][0][:3]])\n",
    "results2 = results2[:4500]\n",
    "print(\"len(results2): \",len(results2))\n",
    "print(\"results2: \",results2)\n",
    "prompt = f\"[INST]`{results2}`. Using the above information, answer the following question factually: {query}. Answer concisely at most in three sentences. Respond in a natural way, like you are having a conversation with a friend.[/INST]\"\n",
    "print(\"=========prompt=============: \")\n",
    "print(prompt)\n",
    "print(\"=========end_of_prompt=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64335ea-b0e2-4c73-8d83-95534c9738b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /nvmefs1/test_user/cache/vector_model/e5-base-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0874cc0f-92b5-4907-8dd4-c5f47cfcbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e9d54-71e1-40fc-af47-c5409cf2ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249e599-6d85-4e80-bef0-1d4f70f09b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_strings = [i['Date'] for i in results['metadatas'][0]]\n",
    "# Your list of datetime strings\n",
    "# date_strings = ['2017-11-21', '2018-03-19', '2022-01-28', '2023-06-20', '2022-04-27']\n",
    "# Step 1: Parse strings into datetime objects\n",
    "date_objects = [datetime.fromisoformat(date_str) for date_str in date_strings]\n",
    "\n",
    "# Step 2: Extract year, month, and day\n",
    "formatted_dates = [dt.strftime('%Y-%m-%d') for dt in date_objects]\n",
    "\n",
    "# Step 3: Sort datetime objects while keeping track of original indices\n",
    "sorted_dates_with_indices = sorted(enumerate(zip(date_objects, formatted_dates)),\n",
    "                                   key=lambda x: x[1][0], reverse=True)\n",
    "\n",
    "# Extract sorted dates and original indices\n",
    "sorted_dates = [date_str for _, (dt, date_str) in sorted_dates_with_indices]\n",
    "original_indices = [index for index, _ in sorted_dates_with_indices]\n",
    "\n",
    "# Print the result\n",
    "print(\"Sorted Dates:\", sorted_dates)\n",
    "print(\"Original Indices:\", original_indices)\n",
    "results_x = [results[\"documents\"][0][original_indices[0]],results[\"documents\"][0][original_indices[1]],results[\"documents\"][0][original_indices[2]]]# get the first three document\n",
    "# await show_sources(results)\n",
    "# print(\"results: \",results)`\n",
    "'''\n",
    "2/6/24 (Andrew): Add limit to ensure that any press release does not exceed >14k. \n",
    "This assumes TitanML API deployed on A100\n",
    "This will decrease when API is deployed no T4.\n",
    "'''\n",
    "results2 = \"\\n\".join(results_x)\n",
    "results2 = results2[:4500]\n",
    "print(\"len(results2): \",len(results2))\n",
    "print(\"results2: \",results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf093136-51ed-40e5-af0a-5bd5ee0a5f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[\"documents\"][0][[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863faf62-0828-4c1d-bd90-df8864b4476f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
