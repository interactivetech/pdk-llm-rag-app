{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12cf3c3e-27b3-4335-bafe-86274b236f03",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Continuous Retrieval Augmentation Generation (RAG)  with HPE MLOPs Platform\n",
    "\n",
    "author: Andrew Mendez, andrew.mendez@hpe.com\n",
    "\n",
    "Version: 0.0.1\n",
    "\n",
    "Date: 12.8.23\n",
    "\n",
    "In this notebook, we see how we can create a RAG system that can automatically update as we add more data. \n",
    "We use MLDM to manage data and pipeline orchestration and TitanML + Chainlit for the user facing application.\n",
    "\n",
    "`Pre-requisites: This demo requires an A100`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3effc75-e431-4f99-b262-bb6797077966",
   "metadata": {},
   "source": [
    "# What are we building\n",
    "We are building a Retrieval Augmented Generation (RAG) system that can continuously improve with more documents.\n",
    "\n",
    "RAG systems is combining vector databases with generative AI systems to reduce LLM hallucinatino with context.\n",
    "\n",
    "<img src=\"./static/rag_ui.PNG\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d754cd-1b8c-42ca-a873-2bbd2153806d",
   "metadata": {},
   "source": [
    "# How will we build this? \n",
    "Using HPE's Machine Learning Operations (MLOps) platform\n",
    "<img src=\"./static/platform_step3.png\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31177cc-1797-4f1b-8253-afc058c2323e",
   "metadata": {},
   "source": [
    "# Overview of MLOPs Pipeline\n",
    "\n",
    "Our ML Pipline consists:\n",
    "* Preproces our documents (we handle xml, csv, and pdf files)\n",
    "* Add our preprocessed documents to a vector database\n",
    "* We then deploy:\n",
    "    * vector database (using ChromaDB)\n",
    "    * an open source pretrained model (Mistral 7B Instruct) as a restful API (using TitanML)\n",
    "    * and a user interface (using chainlit)\n",
    "\n",
    "<img src=\"./static/deploy_rag_pipeline.PNG\" alt=\"Enterprise Machine Learning platform architecture\" width=\"850\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee34e27-902e-4760-a043-4c1afca9df65",
   "metadata": {},
   "source": [
    "## Install pachctl and connect to pachyderm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac31fb90-b55f-4c60-b086-22fef511f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 'pachd-peer.pachyderm.svc.cluster.local:30653' set as active\n",
      "ACTIVE PROJECT                    CREATED      DESCRIPTION\n",
      "       pipeline-finbert           8 months ago Tyler - Legacy FinBERT PDK Demo\n",
      "       pipeline-hpe-fsi-retrieval 8 months ago Tyler - PDK demo of for HPE FSI RAG/Retrieval Demo\n",
      "       pdk-dogs-and-cats          4 months ago Tyler - Legacy Brain Dogs and Cats Demo\n",
      "       pdk-brain-mri              4 months ago Tyler - Legacy Brain MRI PDK Demo\n",
      "       starcoder                  3 months ago Tyler - A fine-tuned version of the huggingface starcoder model with titanML serving\n",
      "       playground_tp              3 months ago Tanguy -  Pachyderm tutorial\n",
      "       object-detection-demo      3 months ago -\n",
      "       Test-TensorRT-LLM          3 months ago Tanguy - Testing model optimization with TensorRT-LLM and deployment with Triton\n",
      "*      deploy-rag                 8 weeks ago  -\n",
      "       deploy-rag-finetune        8 weeks ago  -\n",
      "       test-catdog-pipe-test      7 weeks ago  -\n",
      "       north-pole                 2 weeks ago  -\n",
      "       brain-mri-workshop         2 weeks ago  -\n",
      "       alemor-playground          8 days ago   Alejandro - Testing pipelines\n",
      "       deploy-rag-finetune2       5 days ago   -\n",
      "       pdk-sql-llm                5 days ago   -\n"
     ]
    }
   ],
   "source": [
    "# Connect to deployed pachyderm application\n",
    "!pachctl connect pachd-peer.pachyderm.svc.cluster.local:30653\n",
    "# list current projects\n",
    "!pachctl list project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90946332-d4df-4588-bb9b-f8e0346b7b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPONENT           VERSION             \n",
      "pachctl             2.8.1               \n",
      "pachd               2.8.4               \n"
     ]
    }
   ],
   "source": [
    "!pachctl version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d652c73-0b68-4dfd-a26a-9c89e67829e2",
   "metadata": {},
   "source": [
    "## Create project and set active context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7b903a5-32ca-4cdc-a58b-474535896b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project \"deploy-rag\" already exists\n",
      "project deploy-rag already exists\n",
      "editing the currently active context \"pachd-peer.pachyderm.svc.cluster.local:30653\"\n"
     ]
    }
   ],
   "source": [
    "# Create Pachyderm application\n",
    "!pachctl create project deploy-rag\n",
    "# Set pachctl's active context to the deploy-rag project\n",
    "!pachctl config update context --project deploy-rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732e3b3-e0af-4df5-9688-864e25c3e702",
   "metadata": {},
   "source": [
    "## Create the data repo. \n",
    "* The data repo contains the documents we will ingest into the vector database and RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f811669f-af1f-417b-9ffc-fa90e96467ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl create repo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "093296c1-2074-4de7-9cef-97bd8ee5b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl create repo code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac475ac-a3b6-4684-99cc-a7293e30bae5",
   "metadata": {},
   "source": [
    "upload documents (XML, CSV) to data repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b272952-7eee-43da-9f96-155f52fa6e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pachctl put file data@master: -r -f data/HPE_press_releases/\n",
    "!pachctl put file data@master: -r -f data/HPE_2023_Press_Releases.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735057f-11c5-452e-b788-65d83c6b3f18",
   "metadata": {},
   "source": [
    "Upload code to build RAG application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45f1cdbd-2bb7-4cf1-942c-7e9c27512656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pachctl put file code@master: -r -f src/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88204fcc-5519-4487-8a3b-e5d7dcccc94a",
   "metadata": {},
   "source": [
    "## Process XML Pipeline\n",
    "Here we define our first pipeline artiact, which is to preprocess the xml and csv documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d6a288f-4010-4b1a-a525-21e780674120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing process_xml.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile process_xml.yaml\n",
    "pipeline:\n",
    "    name: 'process_xml'\n",
    "description: 'Extract content in xml files to a csv file'\n",
    "input:\n",
    "    cross:\n",
    "        - pfs: \n",
    "            repo: 'data'\n",
    "            branch: 'master'\n",
    "            glob: '/'\n",
    "        - pfs: \n",
    "            repo: 'code'\n",
    "            branch: 'master'\n",
    "            glob: '/'\n",
    "transform:\n",
    "    image: mendeza/python38_process:0.2\n",
    "    cmd: \n",
    "        - '/bin/sh'\n",
    "    stdin: \n",
    "    - 'python /pfs/code/src/py/process_xmls.py \n",
    "    --xml-directory /pfs/data/HPE_press_releases/ \n",
    "    --pdf-directory /pfs/data/ \n",
    "    --custom-csv-input /pfs/data/HPE_2023_Press_Releases.csv \n",
    "    --out-dir /pfs/out/hpe_press_releases.csv'\n",
    "autoscaling: False\n",
    "pod_patch: >-\n",
    "  [{\"op\": \"add\",\"path\": \"/volumes/-\",\"value\": {\"name\":\n",
    "  \"host-shared\",\"hostpath\": {\"path\":\n",
    "  \"/nvmefs1/\",\"type\": \"Directory\"}}}, {\"op\":\n",
    "  \"add\",\"path\": \"/containers/0/volumeMounts/-\",\"value\": {\"mountPath\":\n",
    "  \"/nvmefs1/\",\"name\": \"host-shared\"}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bf0aa3-2006-4984-afea-4ab9e59399f0",
   "metadata": {},
   "source": [
    "Deploy pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93cc3d9e-7f40-49bf-aeba-26c1a49bef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl create pipeline -f process_xml.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ba870-b4b0-453d-b50e-bb73c4cca421",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Command to download resulting file from process_xml pipeline\n",
    "!pachctl get file process_xml@master:hpe_press_releases.csv > hpe_press_releases.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5694a619-ec00-45e8-98a5-8c4b760ef0d3",
   "metadata": {},
   "source": [
    "## Add documents to vector database Pipeline\n",
    "Here we define our second pipeline artiact, which is to add documents into our vector database. We take the results of the preprocessing step (process_xml) as input, so any new preprocessing runs will trigger this pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca9413e9-d659-4b1c-904f-2b6ca1b7ff95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing add_to_vector_db.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile add_to_vector_db.yaml\n",
    "pipeline:\n",
    "    name: 'add_to_vector_db'\n",
    "description: 'Extract content in xml files to a csv file'\n",
    "input:\n",
    "    cross:\n",
    "        - pfs:\n",
    "            repo: 'process_xml'\n",
    "            branch: 'master'\n",
    "            glob: '/'\n",
    "        - pfs:\n",
    "            repo: 'code'\n",
    "            branch: 'master'\n",
    "            glob: '/'\n",
    "transform:\n",
    "    image: mendeza/python38_process:0.2\n",
    "    cmd: \n",
    "        - '/bin/sh'\n",
    "    stdin: \n",
    "    - 'python /pfs/code/src/py/seed.py --path_to_db /nvmefs1/andrew.mendez/rag_db/\n",
    "    --csv_path /pfs/process_xml/hpe_press_releases.csv\n",
    "    --emb_model_path /nvmefs1/andrew.mendez/chromadb_cache/all-MiniLM-L6-v2'\n",
    "    - 'echo \"$(openssl rand -base64 12)\" > /pfs/out/random_file.txt'\n",
    "    secrets:\n",
    "        - name: pipeline-secret\n",
    "          key: det_master\n",
    "          env_var: DET_MASTER\n",
    "        - name: pipeline-secret\n",
    "          key: det_user\n",
    "          env_var: DET_USER\n",
    "        - name: pipeline-secret\n",
    "          key: det_password\n",
    "          env_var: DET_PASSWORD\n",
    "        - name: pipeline-secret\n",
    "          key: pac_token\n",
    "          env_var: PAC_TOKEN\n",
    "autoscaling: False\n",
    "pod_patch: >-\n",
    "  [{\"op\": \"add\",\"path\": \"/volumes/-\",\"value\": {\"name\":\n",
    "  \"host-shared\",\"hostpath\": {\"path\":\n",
    "  \"/nvmefs1/\",\"type\": \"Directory\"}}}, {\"op\":\n",
    "  \"add\",\"path\": \"/containers/0/volumeMounts/-\",\"value\": {\"mountPath\":\n",
    "  \"/nvmefs1/\",\"name\": \"host-shared\"}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a916049-e412-4c1b-a263-238283dbc614",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl create pipeline -f add_to_vector_db.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d386090-134b-4fa0-8d95-6f99b92b21ac",
   "metadata": {},
   "source": [
    "## Deploy application Pipeline\n",
    "Here we define our 3rd and final pipeline artiact, which is to deploy our finetuned LLM with our RAG system.\n",
    "This step deploys our LLM as a scalable API server using TitanML and our user facing application using Chainlit. \n",
    "MLDM orchestrates allocating GPU resources needed for efficient inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e8e01f6-9e8f-4a28-92ff-8933c889ba11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting deploy.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy.yaml\n",
    "pipeline:\n",
    "    name: 'deploy'\n",
    "description: 'Extract content in xml files to a csv file'\n",
    "input:\n",
    "    cross:\n",
    "        - pfs:\n",
    "            repo: 'add_to_vector_db'\n",
    "            branch: 'master'\n",
    "            glob: '/'\n",
    "        - pfs:\n",
    "            repo: 'code'\n",
    "            branch: 'master'\n",
    "            glob: '/'\n",
    "transform:\n",
    "    image: python:3.8\n",
    "    cmd: \n",
    "        - '/bin/sh'\n",
    "    stdin: \n",
    "        - 'bash /pfs/code/src/scripts/generate_titanml_and_ui_pod_check.sh'\n",
    "autoscaling: False\n",
    "pod_patch: >-\n",
    "  [{\"op\": \"add\",\"path\": \"/volumes/-\",\"value\": {\"name\":\n",
    "  \"host-shared\",\"hostpath\": {\"path\":\n",
    "  \"/nvmefs1/\",\"type\": \"Directory\"}}}, {\"op\":\n",
    "  \"add\",\"path\": \"/containers/0/volumeMounts/-\",\"value\": {\"mountPath\":\n",
    "  \"/nvmefs1/\",\"name\": \"host-shared\"}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8ecde0b-f947-4c53-82a0-680c54ee975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl create pipeline -f deploy.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08db9477-66b1-4f30-bdd4-8b700fb08f70",
   "metadata": {},
   "source": [
    "## Now in the UI at http://10.182.1.50:8080/ , ask it the following question:\n",
    "* \"What is HPE's approach to AI?\n",
    "\n",
    "You will see the application responds with the most relevant document!\n",
    "\n",
    "\n",
    "Lets see how the RAG app will respond on information it doesn know:\n",
    "\n",
    "* \"How long has Antonio Neri been at HPE?\"\n",
    "\n",
    "We will see the RAG does not respond because it does not have this information.\n",
    "\n",
    "Good News: we can add more documents and our system will automatically finetune and reploy the RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623065e-4e5d-4c19-ab8e-264a93751755",
   "metadata": {},
   "source": [
    "# Automatic retraining and deployment of RAG Application\n",
    "Here we see the power of MLDM and MLDE. When we add a press release (in pdf format) abhot how long Antonio Neri has been at HPE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1f9f3-de7e-4815-b87d-d95aa38a0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def pdf(url):\n",
    "    return HTML('<embed src=\"%s\" type=\"application/pdf\" width=\"100%%\" height=\"600px\" />' % url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4721030a-6205-4514-aac9-51440af865d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf('pdf_data/output.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b27983-a713-4252-a718-e0103c2fc955",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pachctl put file data@master: -f pdf_data/output.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66648af1-f7fb-4163-b80e-034ddb1fee54",
   "metadata": {},
   "source": [
    "Lets see how the RAG app will respond on information with the updated document:\n",
    "* \"How long has Antonio Neri been at HPE?\"\n",
    "We see the model gets the answer correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62bbb4-a8e5-44e2-b100-af1905495602",
   "metadata": {},
   "source": [
    "## Clean up workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f3425d2-aa36-436f-aab6-5115f1078a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo deleted.\n",
      "Repo deleted.\n"
     ]
    }
   ],
   "source": [
    "!pachctl delete pipeline deploy\n",
    "!pachctl delete pipeline add_to_vector_db\n",
    "!pachctl delete pipeline process_xml\n",
    "!pachctl delete repo data\n",
    "!pachctl delete repo code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429ba52-fed8-4e89-8e0a-665ea55b77ad",
   "metadata": {},
   "source": [
    "Copy the below command on the management node to free up kubernetes resources for this demo:\n",
    "* `kubectl delete -n pachyderm pod ui-pod && kubectl delete -n pachyderm pod titanml-pod && kubectl delete -n pachyderm svc ui-pod-svc && kubectl delete -n pachyderm svc titanml-pod-svc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eec92c-81d0-4250-b0de-04680c61bffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
